{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianwei/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "# plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model info:\n",
    "\n",
    "# _________________________________________________________________\n",
    "# Layer (type)                 Output Shape              Param #   \n",
    "# =================================================================\n",
    "# conv2d_1 (Conv2D)            (None, 38, 38, 32)        896       \n",
    "# _________________________________________________________________\n",
    "# activation_1 (Activation)    (None, 38, 38, 32)        0         \n",
    "# _________________________________________________________________\n",
    "# conv2d_2 (Conv2D)            (None, 36, 36, 32)        9248      \n",
    "# _________________________________________________________________\n",
    "# activation_2 (Activation)    (None, 36, 36, 32)        0         \n",
    "# _________________________________________________________________\n",
    "# max_pooling2d_1 (MaxPooling2 (None, 18, 18, 32)        0         \n",
    "# _________________________________________________________________\n",
    "# dropout_1 (Dropout)          (None, 18, 18, 32)        0         \n",
    "# _________________________________________________________________\n",
    "# conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
    "# _________________________________________________________________\n",
    "# activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
    "# _________________________________________________________________\n",
    "# conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
    "# _________________________________________________________________\n",
    "# activation_4 (Activation)    (None, 14, 14, 64)        0         \n",
    "# _________________________________________________________________\n",
    "# max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
    "# _________________________________________________________________\n",
    "# dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
    "# _________________________________________________________________\n",
    "# flatten_1 (Flatten)          (None, 3136)              0         \n",
    "# _________________________________________________________________\n",
    "# dense_1 (Dense)              (None, 256)               803072    \n",
    "# _________________________________________________________________\n",
    "# activation_5 (Activation)    (None, 256)               0         \n",
    "# _________________________________________________________________\n",
    "# dropout_3 (Dropout)          (None, 256)               0         \n",
    "# _________________________________________________________________\n",
    "# dense_2 (Dense)              (None, 10)                2570      \n",
    "# _________________________________________________________________\n",
    "# activation_6 (Activation)    (None, 10)                0         \n",
    "# =================================================================\n",
    "# Total params: 871,210\n",
    "# Trainable params: 871,210\n",
    "# Non-trainable params: 0\n",
    "# _________________________________________________________________\n",
    "\n",
    "# 0.9M params, about 4MB (keras 10MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_evidence(logits): \n",
    "    return tf.exp(tf.clip_by_value(logits,-10,10))\n",
    "\n",
    "#### KL Divergence calculator\n",
    "\n",
    "def KL(alpha, K):\n",
    "    beta=tf.constant(np.ones((1,K)),dtype=tf.float32)\n",
    "    S_alpha = tf.reduce_sum(alpha,axis=1,keepdims=True)\n",
    "    \n",
    "    KL = tf.reduce_sum((alpha - beta)*(tf.digamma(alpha)-tf.digamma(S_alpha)),axis=1,keepdims=True) + \\\n",
    "         tf.lgamma(S_alpha) - tf.reduce_sum(tf.lgamma(alpha),axis=1,keepdims=True) + \\\n",
    "         tf.reduce_sum(tf.lgamma(beta),axis=1,keepdims=True) - tf.lgamma(tf.reduce_sum(beta,axis=1,keepdims=True))\n",
    "    return KL\n",
    "\n",
    "def mse_loss(p, alpha, K, global_step, annealing_step): \n",
    "    S = tf.reduce_sum(alpha, axis=1, keep_dims=True) \n",
    "    E = alpha - 1\n",
    "    m = alpha / S\n",
    "    \n",
    "    A = tf.reduce_sum((p-m)**2, axis=1, keep_dims=True) \n",
    "    B = tf.reduce_sum(alpha*(S-alpha)/(S*S*(S+1)), axis=1, keep_dims=True) \n",
    "    \n",
    "    annealing_coef = tf.minimum(1.0,tf.cast(global_step/annealing_step,tf.float32))\n",
    "    \n",
    "    alp = E*(1-p) + 1 \n",
    "    C =  annealing_coef * KL(alp, K)\n",
    "    return (A + B) + C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-f516c369830a>:14: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "# new network:\n",
    "\n",
    "# NETWORK PARAMETERS\n",
    "data_w = 40\n",
    "data_h = 40\n",
    "n_classes = 10\n",
    "n_filters_1 = 32\n",
    "n_filters_2 = 64\n",
    "d_filter = 3\n",
    "p_drop_1 = 0.25\n",
    "p_drop_2 = 0.50\n",
    "\n",
    "batch_size = 256\n",
    "nb_epoch = 20\n",
    "\n",
    "K= n_classes\n",
    "num_channels = 3\n",
    "num_labels = n_classes\n",
    "\n",
    "lmb = 0.00\n",
    "omega = 1.0\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=omega)\n",
    "\n",
    "\n",
    "# new network:\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None,data_w,data_h,num_channels], name = 'input')\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels], name = 'label')\n",
    "\n",
    "keep_prob = tf.placeholder(dtype=tf.float32, name = 'dropout_rate')\n",
    "global_step = tf.Variable(initial_value=0, name='global_step', trainable=False)\n",
    "annealing_step = tf.placeholder(dtype=tf.int32, name = 'annealing_step') \n",
    "\n",
    "### conv module\n",
    "\n",
    "# Convolutional Layer #1\n",
    "conv1 = tf.layers.conv2d(\n",
    "    inputs=X,\n",
    "    filters=32,\n",
    "    strides=(1, 1),\n",
    "    kernel_size=[3, 3],\n",
    "    kernel_regularizer=regularizer,\n",
    "    padding=\"valid\"\n",
    "    )\n",
    "conv1_act = tf.nn.relu( conv1 )\n",
    "# pool1 = tf.layers.max_pooling2d(inputs=act1, pool_size=[3, 3], strides=3)\n",
    "# dropout1 = tf.layers.dropout(\n",
    "#     inputs=pool1, rate=0.1)\n",
    "\n",
    "# Convolutional Layer #2\n",
    "conv2 = tf.layers.conv2d(\n",
    "    inputs=conv1_act,\n",
    "    filters=32,\n",
    "    strides=(1, 1),\n",
    "    kernel_size=[3, 3],\n",
    "    kernel_regularizer=regularizer,\n",
    "    padding=\"valid\"\n",
    "    )\n",
    "# bn2 = tf.layers.batch_normalization(\n",
    "#     conv2,\n",
    "#     axis=-1\n",
    "#     )\n",
    "conv2_act = tf.nn.relu( conv2 )\n",
    "conv2_mp = tf.layers.max_pooling2d(inputs=conv2_act, pool_size=[2, 2], strides=2)\n",
    "dpout1 = tf.layers.dropout(\n",
    "    inputs=conv2_mp, rate= p_drop_1)\n",
    "\n",
    "\n",
    "\n",
    "# Convolutional Layer #3\n",
    "conv3 = tf.layers.conv2d(\n",
    "    inputs=dpout1,\n",
    "    filters=64,\n",
    "    strides=(1, 1),\n",
    "    kernel_size=[3, 3],\n",
    "    kernel_regularizer=regularizer,\n",
    "    padding=\"valid\"\n",
    "    )\n",
    "conv3_act = tf.nn.relu( conv3 )\n",
    "\n",
    "# Convolutional Layer #4\n",
    "conv4 = tf.layers.conv2d(\n",
    "    inputs=conv1_act,\n",
    "    filters=64,\n",
    "    strides=(1, 1),\n",
    "    kernel_size=[3, 3],\n",
    "    kernel_regularizer=regularizer,\n",
    "    padding=\"valid\"\n",
    "    )\n",
    "# bn2 = tf.layers.batch_normalization(\n",
    "#     conv2,\n",
    "#     axis=-1\n",
    "#     )\n",
    "conv4_act = tf.nn.relu( conv4 )\n",
    "conv4_mp = tf.layers.max_pooling2d(inputs=conv4_act, pool_size=[2, 2], strides=2)\n",
    "dpout2 = tf.layers.dropout(\n",
    "    inputs=conv4_mp, rate= p_drop_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### modify dimensions\n",
    "shape = dpout2.get_shape().as_list()\n",
    "flat1 = tf.reshape(dpout2, [-1, shape[1] * shape[2]* shape[3]])\n",
    "\n",
    "\n",
    "\n",
    "### dense module\n",
    "\n",
    "fc1 = tf.layers.dense(inputs=flat1, \n",
    "                          kernel_regularizer=regularizer,\n",
    "                          units=256)\n",
    "fc1_act = tf.nn.relu( fc1 )\n",
    "dpout3 = tf.layers.dropout(\n",
    "    inputs=fc1_act, rate= p_drop_2)\n",
    "\n",
    "# Logits Layer\n",
    "logits = tf.layers.dense(inputs=dpout3, \n",
    "                         kernel_regularizer=regularizer,\n",
    "                         units=n_classes,\n",
    "                         name = 'logits_tensor')\n",
    "\n",
    "\n",
    "y_ = tf.nn.softmax(logits,name=\"softmax_tensor\")\n",
    "\n",
    "\n",
    "prediction = tf.argmax(logits, 1)\n",
    "\n",
    "\n",
    "\n",
    "########### EDL extension ###########\n",
    " \n",
    "logits2evidence =  exp_evidence ############ modify this function:  relu_evidence  exp_evidence softplus\n",
    "\n",
    "evidence = logits2evidence(logits)\n",
    "alpha = evidence + 1\n",
    "\n",
    "u = K / tf.reduce_sum(alpha, axis=1, keepdims=True)\n",
    "\n",
    "prob = alpha/tf.reduce_sum(alpha, 1, keepdims=True) \n",
    "\n",
    "loss_function = mse_loss  ########### use 5th MSE loss equ: loss_eq5, loss_eq4, loss_eq3, mse_loss\n",
    "\n",
    "loss = tf.reduce_mean(loss_function(Y, alpha, K, global_step, annealing_step))\n",
    "l2_loss = tf.losses.get_regularization_loss() * lmb\n",
    "loss_func = loss + l2_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss_func, global_step=global_step)\n",
    "\n",
    "match = tf.reshape(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1)), tf.float32),(-1,1))\n",
    "accuracy = tf.reduce_mean(match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from SaveModel/best_model.ckpt\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "# best_model_path = \"save_model/model_test1.ckpt\"\n",
    "best_model_path = \"SaveModel/best_model.ckpt\"\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "session =  tf.Session()\n",
    "saver.restore(session, best_model_path)\n",
    "print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading graph from file\n",
    "\n",
    "# saver = tf.train.import_meta_graph('save_model/model_test2.ckpt.meta')\n",
    "\n",
    "# session =  tf.Session()\n",
    "\n",
    "# saver.restore(session,tf.train.latest_checkpoint(checkpoint_dir = 'save_model/'))\n",
    "# print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_class):\n",
    "    n_labels = len(labels)\n",
    "#     n_unique_labels = len(np.unique(labels))\n",
    "    n_unique_labels = num_class\n",
    "    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading data\n",
      "Data loaded\n",
      "(54154, 40, 40, 3)\n",
      "(54154, 10)\n",
      "For batch of size 256: \n",
      " 211 batches in test\n"
     ]
    }
   ],
   "source": [
    "## GET DATA TO WORK ON\n",
    "print(\"Start loading data\")\n",
    "\n",
    "fd = open(\"data_x.pkl\", 'rb')\n",
    "fd2 = open(\"data_y.pkl\", 'rb')\n",
    "features = pickle.load(fd)\n",
    "labels = pickle.load(fd2)\n",
    "\n",
    "print(\"Data loaded\")\n",
    "\n",
    "# all testing data\n",
    "X_test = features[:]\n",
    "Y_test = labels[:]\n",
    "\n",
    "X_test = X_test.astype('float32')\n",
    "X_test /= 255\n",
    "\n",
    "## one hot encoding\n",
    "Y_test = one_hot_encode(Y_test, 10)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "n_batches_test = Y_test.shape[0]//batch_size\n",
    "print('For batch of size %d: \\n %d batches in test'%(batch_size, n_batches_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 211 - 100% -0.901015) Testing:\t  Loss: 0.0010 \t Accuracy: 0.9293\n"
     ]
    }
   ],
   "source": [
    "pred_y_list = np.zeros(0)\n",
    "\n",
    "acc_list = []\n",
    "loss_list = []\n",
    "\n",
    "#       Performance on testing dataset:\n",
    "for i in range(n_batches_test):\n",
    "    if i == n_batches_test-1:\n",
    "        batch_x = X_test[i * batch_size:, :, :, :]\n",
    "        batch_y = Y_test[i * batch_size:, :]\n",
    "    else:\n",
    "        offset = (i * batch_size) % (Y_test.shape[0] - batch_size)\n",
    "        batch_x = X_test[offset:(offset + batch_size), :, :, :]\n",
    "        batch_y = Y_test[offset:(offset + batch_size), :]\n",
    "\n",
    "#     logits, y_pred = session.run([logits, prediction], feed_dict={X: batch_x, Y : batch_y})\n",
    "    y_pred, acc, c = session.run([prediction, accuracy, loss_func], feed_dict={X: batch_x, Y : batch_y, keep_prob:1.,  annealing_step:100*n_batches_test})\n",
    "\n",
    "    print('epoch %d - %d%% -%f) '% (i+1, (100*(i+1))//n_batches_test, acc), end='\\r' if i<n_batches_test-1 else '')\n",
    "#     y_pred = np.argmax(logits, axis=1)\n",
    "#     pred_y_list.append(y_pred)\n",
    "    pred_y_list = np.concatenate([pred_y_list, y_pred])\n",
    "    \n",
    "    acc_list.append(acc)\n",
    "    loss_list.append(c)\n",
    "#     test_acc = np.array(np.array(acc_list).mean())\n",
    "#     test_loss = np.array(np.array(loss_list).sum())\n",
    "    \n",
    "\n",
    "test_acc = np.array(np.array(acc_list).mean())\n",
    "test_loss = np.array(np.array(loss_list).sum())\n",
    "print('Testing:\\t  Loss: %2.4f \\t Accuracy: %2.4f' % (test_loss/Y_test.shape[0], test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing data: 0.9292757691029286\n",
      "[[6409    3   72  136  129    4   22   29   74  112]\n",
      " [  11 1579   23   12   19    8    0   10    2   15]\n",
      " [  24    5 6567   99   19   20   12   17   25  135]\n",
      " [  42    3  121 4915   19   14   24   20   30   84]\n",
      " [  34   16   73   75 5613   20   20  101   37   76]\n",
      " [ 225   15   52  182   46 5758  162  248   50  134]\n",
      " [   6    0   11   29    9    3  742    0   13    5]\n",
      " [  26    1   12   20   15    3    2 6064    2   56]\n",
      " [  11    5   42   35    3    6    3   11 6202   16]\n",
      " [  63    8  159  135   31   28   10   23   68 6475]]\n",
      "the mean-f1 score: 0.92\n"
     ]
    }
   ],
   "source": [
    "y_true = np.argmax(Y_test, 1)\n",
    "y_pred =pred_y_list\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('Accuracy on testing data:',sum(y_pred==y_true)/y_true.shape[0])\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(cf_matrix)\n",
    "class_wise_f1 = np.round(f1_score(y_true, y_pred, average=None)*100)*0.01\n",
    "print('the mean-f1 score: {:.2f}'.format(np.mean(class_wise_f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About classes:\n",
    "# 0\tair_conditioner\n",
    "# 1\tcar_horn\n",
    "# 2\tchildren_playing\n",
    "# 3\tdog_bark\n",
    "# 4\tdrilling\n",
    "# 5\tengine_idling\n",
    "# 6\tgun_shot\n",
    "# 7\tjackhammer\n",
    "# 8\tsiren\n",
    "# 9\tstreet_music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 211 - 100% -0.901015) Testing:\t  Loss: 0.0010 \t Accuracy: 0.9293\n"
     ]
    }
   ],
   "source": [
    "pred_y_list = np.zeros(0)\n",
    "uncertainty_y_list = np.zeros([0,1])\n",
    "\n",
    "acc_list = []\n",
    "loss_list = []\n",
    "\n",
    "#       Performance on testing dataset:\n",
    "for i in range(n_batches_test):\n",
    "    if i == n_batches_test-1:\n",
    "        batch_x = X_test[i * batch_size:, :, :, :]\n",
    "        batch_y = Y_test[i * batch_size:, :]\n",
    "    else:\n",
    "        offset = (i * batch_size) % (Y_test.shape[0] - batch_size)\n",
    "        batch_x = X_test[offset:(offset + batch_size), :, :, :]\n",
    "        batch_y = Y_test[offset:(offset + batch_size), :]\n",
    "\n",
    "#     logits, y_pred = session.run([logits, prediction], feed_dict={X: batch_x, Y : batch_y})\n",
    "    y_pred, acc, c, uncertainty = session.run([prediction, accuracy, loss_func, u], feed_dict={X: batch_x, Y : batch_y, keep_prob:1.,  annealing_step:100*n_batches_test})\n",
    "\n",
    "    print('epoch %d - %d%% -%f) '% (i+1, (100*(i+1))//n_batches_test, acc), end='\\r' if i<n_batches_test-1 else '')\n",
    "#     y_pred = np.argmax(logits, axis=1)\n",
    "#     pred_y_list.append(y_pred)\n",
    "    pred_y_list = np.concatenate([pred_y_list, y_pred])\n",
    "    uncertainty_y_list = np.concatenate([uncertainty_y_list, uncertainty])\n",
    "    \n",
    "    acc_list.append(acc)\n",
    "    loss_list.append(c)\n",
    "#     test_acc = np.array(np.array(acc_list).mean())\n",
    "#     test_loss = np.array(np.array(loss_list).sum())\n",
    "    \n",
    "\n",
    "test_acc = np.array(np.array(acc_list).mean())\n",
    "test_loss = np.array(np.array(loss_list).sum())\n",
    "print('Testing:\\t  Loss: %2.4f \\t Accuracy: %2.4f' % (test_loss/Y_test.shape[0], test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 0.0 \t [0.00045379]\n",
      "0 \t 0.0 \t [0.00045379]\n",
      "0 \t 0.0 \t [0.00045379]\n",
      "0 \t 0.0 \t [0.00045379]\n",
      "0 \t 0.0 \t [0.00045379]\n",
      "0 \t 0.0 \t [0.00045379]\n",
      "0 \t 0.0 \t [0.00045379]\n",
      "3 \t 3.0 \t [0.12549192]\n",
      "3 \t 3.0 \t [0.00045379]\n",
      "5 \t 9.0 \t [0.9929685] \t!!!\n",
      "5 \t 9.0 \t [0.99995458] \t!!!\n",
      "5 \t 9.0 \t [0.99995458] \t!!!\n",
      "5 \t 7.0 \t [0.99995458] \t!!!\n",
      "5 \t 9.0 \t [0.99995232] \t!!!\n",
      "5 \t 9.0 \t [0.99995458] \t!!!\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "# for i in range(pred_y_list.shape[0]):\n",
    "    if np.argmax(Y_test[i]) != pred_y_list[i]:\n",
    "        print(np.argmax(Y_test[i]), '\\t', pred_y_list[i], '\\t', uncertainty_y_list[i], '\\t!!!')\n",
    "    else:\n",
    "        print(np.argmax(Y_test[i]), '\\t', pred_y_list[i], '\\t', uncertainty_y_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AUDIO DATA PROCESSING\n",
    "import os\n",
    "import librosa\n",
    "import pickle\n",
    "\n",
    "window_size = 512\n",
    "## This for mel spectogram resolution\n",
    "n_bands = 60\n",
    "n_mfcc = 40\n",
    "n_frames = 40\n",
    "\n",
    "def windows(data, n_frames):\n",
    "    ws = window_size * (n_frames - 1)\n",
    "    start = 0\n",
    "    while start < len(data):\n",
    "        yield start, start + ws, ws\n",
    "        start += (ws / 2)\n",
    "        ## OVERLAP OF 50%\n",
    "## END windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(work_dir):\n",
    "#     work_dir = \"gta_sound\"\n",
    "    raw_features = []\n",
    "    _labels = []\n",
    "    cnt = 0\n",
    "\n",
    "    print(\"Working on dir: \", work_dir)\n",
    "\n",
    "\n",
    "    for fs in os.listdir(work_dir ):\n",
    "    #     if \".wav\" not in fs: continue\n",
    "        # print(\"Try Loading file: \", fs)\n",
    "        sound_clip, sr = librosa.load(work_dir + \"/\" + fs)\n",
    "        label = 6\n",
    "        print(cnt, \"Try Loading file: \", fs, \" class: \", label)\n",
    "        cnt += 1\n",
    "        ## Work of file bacthes\n",
    "        for (start, end, ws) in windows(sound_clip, n_frames):\n",
    "            ## Get the sound part\n",
    "            signal = sound_clip[int(start): int(end)]\n",
    "            if len(signal) == ws:\n",
    "                mfcc_spec = librosa.feature.mfcc(signal, n_mfcc=n_mfcc, n_mels=n_bands)\n",
    "    #             print(mfcc_spec.shape)\n",
    "                mfcc_spec = mfcc_spec.T.flatten()[:, np.newaxis].T\n",
    "    #             print(mfcc_spec.shape)\n",
    "                raw_features.append(mfcc_spec)\n",
    "                _labels.append(label)\n",
    "\n",
    "    print(\"Loaded \", cnt, \" files\")\n",
    "    ## Add a new dimension\n",
    "    raw_features = np.asarray(raw_features).reshape(len(raw_features), n_mfcc, n_frames, 1)\n",
    "\n",
    "\n",
    "\n",
    "    ## Concate 2 elements on axis=3\n",
    "    _features = np.concatenate((raw_features, np.zeros(np.shape(raw_features))), axis=3)\n",
    "\n",
    "    _features = np.concatenate((_features, np.zeros(np.shape(raw_features))), axis=3)\n",
    "\n",
    "\n",
    "    for i in range(len(_features)):\n",
    "        _features[i, :, :, 1] = librosa.feature.delta(order=1, data=_features[i, :, :, 0])\n",
    "        _features[i, :, :, 2] = librosa.feature.delta(order=2, data=_features[i, :, :, 0])\n",
    "\n",
    "    # normalize, one-hot data\n",
    "    test_x = _features\n",
    "    test_x = test_x.astype('float32')\n",
    "    test_x /= 255\n",
    "\n",
    "    test_y = one_hot_encode(np.array(_labels), 10)\n",
    "    print(test_x.shape, test_y.shape)\n",
    "    \n",
    "    return test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on dir:  gta_sound\n",
      "0 Try Loading file:  pistol.wav  class:  6\n",
      "1 Try Loading file:  sniper_m14.wav  class:  6\n",
      "2 Try Loading file:  pistol_magnum.wav  class:  6\n",
      "3 Try Loading file:  rifle_ak47_single_fire.wav  class:  6\n",
      "4 Try Loading file:  sniper_g3sg1.wav  class:  6\n",
      "5 Try Loading file:  rifle_m16_single_fire.wav  class:  6\n",
      "6 Try Loading file:  rifle_m60_single_fire.wav  class:  6\n",
      "7 Try Loading file:  smg_uzi_singl_fire.wav  class:  6\n",
      "Loaded  8  files\n",
      "(17, 40, 40, 3) (17, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianwei/anaconda3/lib/python3.6/site-packages/scipy/signal/_arraytools.py:45: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  b = a[a_slice]\n"
     ]
    }
   ],
   "source": [
    "# work_dir = 'gun_shot'\n",
    "work_dir = 'gta_sound'\n",
    "\n",
    "test_x, test_y = data_processing(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fd = open(\"gun_x.pkl\", 'wb')\n",
    "# pickle.dump(_features, fd)\n",
    "# fd2 = open(\"gun_y.pkl\", 'wb')\n",
    "# pickle.dump(one_hot_encode(np.array(_labels), 10), fd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## GET DATA TO WORK ON\n",
    "# print(\"Start loading data\")\n",
    "\n",
    "# fd = open(\"org_gun_x.pkl\", 'rb')\n",
    "# fd2 = open(\"org_gun_y.pkl\", 'rb')\n",
    "# test_x = np.array(pickle.load(fd))\n",
    "# test_x = test_x.astype('float32')\n",
    "# test_x /= 255\n",
    "\n",
    "# test_y = np.array(pickle.load(fd2))\n",
    "# test_y = one_hot_encode(np.array(test_y), 10)\n",
    "# print(\"Data loaded\")\n",
    "# print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.8235294\n"
     ]
    }
   ],
   "source": [
    "y_pred, acc, c, uncertainty = session.run([prediction, accuracy, loss_func, u], feed_dict={X: test_x, Y : test_y, keep_prob:1.,  annealing_step:100*n_batches_test})\n",
    "print('Accuracy is: ',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True \t Predict \t Uncertainty \t Match\n",
      "6  \t 6 \t\t  [0.00045379]\n",
      "6  \t 6 \t\t  [0.00045379]\n",
      "6  \t 6 \t\t  [0.00045379]\n",
      "6  \t 6 \t\t  [0.00045379]\n",
      "6  \t 6 \t\t  [0.00045379]\n",
      "6  \t 3 \t\t  [0.00045379]     !!!\n",
      "6  \t 6 \t\t  [0.00045379]\n",
      "6  \t 6 \t\t  [0.12549192]\n",
      "6  \t 4 \t\t  [0.00045379]     !!!\n",
      "6  \t 6 \t\t  [0.9929685]\n",
      "6  \t 6 \t\t  [0.99995458]\n",
      "6  \t 3 \t\t  [0.99995458]     !!!\n",
      "6  \t 6 \t\t  [0.99995458]\n",
      "6  \t 6 \t\t  [0.99995232]\n",
      "6  \t 6 \t\t  [0.99995458]\n",
      "6  \t 6 \t\t  [0.99995458]\n",
      "6  \t 6 \t\t  [0.00045379]\n"
     ]
    }
   ],
   "source": [
    "print('True \\t Predict \\t Uncertainty \\t Match')\n",
    "for i in range(y_pred.shape[0]):\n",
    "# for i in range(pred_y_list.shape[0]):\n",
    "    if np.argmax(test_y[i]) != y_pred[i]:\n",
    "        print(np.argmax(test_y[i]), ' \\t', y_pred[i], '\\t\\t ', uncertainty_y_list[i], '    !!!')\n",
    "    else:\n",
    "        print(np.argmax(test_y[i]), ' \\t', y_pred[i], '\\t\\t ', uncertainty_y_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About classes:\n",
    "# 0\tair_conditioner\n",
    "# 1\tcar_horn\n",
    "# 2\tchildren_playing\n",
    "# 3\tdog_bark\n",
    "# 4\tdrilling\n",
    "# 5\tengine_idling\n",
    "# 6\tgun_shot\n",
    "# 7\tjackhammer\n",
    "# 8\tsiren\n",
    "# 9\tstreet_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
