{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianwei/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Logit to evidence converters - activation functions (they have to produce non-negative outputs for the uncertaintyuncertainity process)\n",
    "\n",
    "def relu_evidence(logits):\n",
    "    return tf.nn.relu(logits)\n",
    "\n",
    "def exp_evidence(logits):                  # tune the parameters  1000\n",
    "    return tf.exp(logits/1)\n",
    "\n",
    "def softplus(logits):\n",
    "    return tf.keras.activations.softplus(logits)\n",
    "  \n",
    "def relu6_evidence(logits):\n",
    "    return tf.nn.relu6(logits)\n",
    "  \n",
    "def softsign_evidence(logits):\n",
    "    return tf.nn.softsign(logits)\n",
    "\n",
    "  \n",
    "#### KL Divergence calculator\n",
    "\n",
    "def KL(alpha, K):\n",
    "    beta=tf.constant(np.ones((1,K)),dtype=tf.float32)\n",
    "    S_alpha = tf.reduce_sum(alpha,axis=1,keepdims=True)\n",
    "    \n",
    "    KL = tf.reduce_sum((alpha - beta)*(tf.digamma(alpha)-tf.digamma(S_alpha)),axis=1,keepdims=True) + \\\n",
    "         tf.lgamma(S_alpha) - tf.reduce_sum(tf.lgamma(alpha),axis=1,keepdims=True) + \\\n",
    "         tf.reduce_sum(tf.lgamma(beta),axis=1,keepdims=True) - tf.lgamma(tf.reduce_sum(beta,axis=1,keepdims=True))\n",
    "    return KL\n",
    "\n",
    "\n",
    "##### Loss functions (there are three different one defined in the papaer)\n",
    "\n",
    "def loss_eq5(p, alpha, K, global_step, annealing_step):  # MSE\n",
    "    S = tf.reduce_sum(alpha, axis=1, keepdims=True)\n",
    "    loglikelihood = tf.reduce_sum((p-(alpha/S))**2, axis=1, keepdims=True) + tf.reduce_sum(alpha*(S-alpha)/(S*S*(S+1)), axis=1, keepdims=True)\n",
    "    KL_reg =  tf.minimum(1.0, tf.cast(global_step/annealing_step, tf.float32)) * KL((alpha - 1)*(1-p) + 1 , K)\n",
    "    return loglikelihood + KL_reg\n",
    "\n",
    "def loss_eq4(p, alpha, K, global_step, annealing_step):   #  expected cross entropy loss\n",
    "    loglikelihood = tf.reduce_mean(tf.reduce_sum(p * (tf.digamma(tf.reduce_sum(alpha, axis=1, keepdims=True)) - tf.digamma(alpha)), 1, keepdims=True))\n",
    "    KL_reg =  tf.minimum(1.0, tf.cast(global_step/annealing_step, tf.float32)) * KL((alpha - 1)*(1-p) + 1 , K)\n",
    "    return loglikelihood + KL_reg\n",
    "\n",
    "def loss_eq3(p, alpha, K, global_step, annealing_step):\n",
    "    loglikelihood = tf.reduce_mean(tf.reduce_sum(p * (tf.log(tf.reduce_sum(alpha, axis=1, keepdims=True)) - tf.log(alpha)), 1, keepdims=True))\n",
    "    KL_reg =  tf.minimum(1.0, tf.cast(global_step/annealing_step, tf.float32)) * KL((alpha - 1)*(1-p) + 1 , K)\n",
    "    return loglikelihood + KL_reg\n",
    "\n",
    "def mse_loss(p, alpha, K, global_step, annealing_step): \n",
    "    S = tf.reduce_sum(alpha, axis=1, keep_dims=True) \n",
    "    E = alpha - 1\n",
    "    m = alpha / S\n",
    "    \n",
    "    A = tf.reduce_sum((p-m)**2, axis=1, keep_dims=True) \n",
    "    B = tf.reduce_sum(alpha*(S-alpha)/(S*S*(S+1)), axis=1, keep_dims=True) \n",
    "    \n",
    "    annealing_coef = tf.minimum(1.0,tf.cast(global_step/annealing_step,tf.float32))\n",
    "    \n",
    "    alp = E*(1-p) + 1 \n",
    "    C =  annealing_coef * KL(alp)\n",
    "    return (A + B) + C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading data\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## GET DATA TO WORK ON\n",
    "print(\"Start loading data\")\n",
    "\n",
    "fd = open(\"data_x.pkl\", 'rb')\n",
    "fd2 = open(\"data_y.pkl\", 'rb')\n",
    "features = pickle.load(fd)\n",
    "labels = pickle.load(fd2)\n",
    "\n",
    "print(\"Data loaded\")\n",
    "\n",
    "# TRAIN - TEST\n",
    "p_train = 0.8\n",
    "\n",
    "rnd_indices = np.random.rand(len(labels)) < p_train\n",
    "X_train = features[rnd_indices]\n",
    "Y_train = labels[rnd_indices]\n",
    "X_test = features[~rnd_indices]\n",
    "Y_test = labels[~rnd_indices]\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "\n",
    "## FIX FOR KERAS\n",
    "# Y_train = Y_train.reshape((-1, 1))\n",
    "# Y_test = Y_test.reshape((-1, 1))\n",
    "\n",
    "## one hot encoding\n",
    "Y_train = one_hot_encode(Y_train)\n",
    "Y_test = one_hot_encode(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43258, 40, 40, 3)\n",
      "(43258, 10)\n",
      "(10896, 40, 40, 3)\n",
      "(10896, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For batch of size 32: \n",
      " 1351 batches in train \n",
      " 340 batches in test\n"
     ]
    }
   ],
   "source": [
    "# NETWORK PARAMETERS\n",
    "data_w = 40\n",
    "data_h = 40\n",
    "n_classes = 10\n",
    "n_filters_1 = 32\n",
    "n_filters_2 = 64\n",
    "d_filter = 3\n",
    "p_drop_1 = 0.25\n",
    "p_drop_2 = 0.50\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "nb_epoch = 20\n",
    "\n",
    "\n",
    "K= n_classes\n",
    "num_channels = 3\n",
    "num_labels = n_classes\n",
    "\n",
    "n_batches = Y_train.shape[0]//batch_size\n",
    "n_batches_test = Y_test.shape[0]//batch_size\n",
    "print('For batch of size %d: \\n %d batches in train \\n %d batches in test'%(batch_size, n_batches, n_batches_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmb = 0.1\n",
    "omega = 1.0\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new network:\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None,data_w,data_h,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "global_step = tf.Variable(initial_value=0, name='global_step', trainable=False)\n",
    "annealing_step = tf.placeholder(dtype=tf.int32) \n",
    "\n",
    "### conv module\n",
    "\n",
    "# Convolutional Layer #1\n",
    "conv1 = tf.layers.conv2d(\n",
    "    inputs=X,\n",
    "    filters=32,\n",
    "    strides=(1, 1),\n",
    "    kernel_size=[3, 3],\n",
    "    kernel_regularizer=regularizer,\n",
    "    padding=\"valid\"\n",
    "    )\n",
    "conv1_act = tf.nn.relu( conv1 )\n",
    "# pool1 = tf.layers.max_pooling2d(inputs=act1, pool_size=[3, 3], strides=3)\n",
    "# dropout1 = tf.layers.dropout(\n",
    "#     inputs=pool1, rate=0.1)\n",
    "\n",
    "# Convolutional Layer #2\n",
    "conv2 = tf.layers.conv2d(\n",
    "    inputs=conv1_act,\n",
    "    filters=32,\n",
    "    strides=(1, 1),\n",
    "    kernel_size=[3, 3],\n",
    "    kernel_regularizer=regularizer,\n",
    "    padding=\"valid\"\n",
    "    )\n",
    "# bn2 = tf.layers.batch_normalization(\n",
    "#     conv2,\n",
    "#     axis=-1\n",
    "#     )\n",
    "conv2_act = tf.nn.relu( conv2 )\n",
    "conv2_mp = tf.layers.max_pooling2d(inputs=conv2_act, pool_size=[2, 2], strides=2)\n",
    "dpout1 = tf.layers.dropout(\n",
    "    inputs=conv2_mp, rate= p_drop_1)\n",
    "\n",
    "\n",
    "\n",
    "# Convolutional Layer #3\n",
    "conv3 = tf.layers.conv2d(\n",
    "    inputs=dpout1,\n",
    "    filters=64,\n",
    "    strides=(1, 1),\n",
    "    kernel_size=[3, 3],\n",
    "    kernel_regularizer=regularizer,\n",
    "    padding=\"valid\"\n",
    "    )\n",
    "conv3_act = tf.nn.relu( conv3 )\n",
    "\n",
    "# Convolutional Layer #4\n",
    "conv4 = tf.layers.conv2d(\n",
    "    inputs=conv1_act,\n",
    "    filters=64,\n",
    "    strides=(1, 1),\n",
    "    kernel_size=[3, 3],\n",
    "    kernel_regularizer=regularizer,\n",
    "    padding=\"valid\"\n",
    "    )\n",
    "# bn2 = tf.layers.batch_normalization(\n",
    "#     conv2,\n",
    "#     axis=-1\n",
    "#     )\n",
    "conv4_act = tf.nn.relu( conv4 )\n",
    "conv4_mp = tf.layers.max_pooling2d(inputs=conv4_act, pool_size=[2, 2], strides=2)\n",
    "dpout2 = tf.layers.dropout(\n",
    "    inputs=conv4_mp, rate= p_drop_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### modify dimensions\n",
    "shape = dpout2.get_shape().as_list()\n",
    "flat1 = tf.reshape(dpout2, [-1, shape[1] * shape[2]* shape[3]])\n",
    "\n",
    "\n",
    "\n",
    "### dense module\n",
    "\n",
    "fc1 = tf.layers.dense(inputs=flat1, \n",
    "                          kernel_regularizer=regularizer,\n",
    "                          units=256)\n",
    "fc1_act = tf.nn.relu( fc1 )\n",
    "dpout3 = tf.layers.dropout(\n",
    "    inputs=fc1_act, rate= p_drop_2)\n",
    "\n",
    "# Logits Layer\n",
    "logits = tf.layers.dense(inputs=dpout3, \n",
    "                         kernel_regularizer=regularizer,\n",
    "                         units=n_classes)\n",
    "\n",
    "\n",
    "y_ = tf.nn.softmax(logits,name=\"softmax_tensor\")\n",
    "\n",
    "\n",
    "prediction = tf.argmax(logits, 1)\n",
    "\n",
    "\n",
    "\n",
    "########### EDL extension ###########\n",
    " \n",
    "logits2evidence = relu_evidence  ############ modify this function:   exp_evidence softplus\n",
    "\n",
    "evidence = logits2evidence(logits)\n",
    "alpha = evidence + 1\n",
    "\n",
    "u = K / tf.reduce_sum(alpha, axis=1, keepdims=True)\n",
    "\n",
    "prob = alpha/tf.reduce_sum(alpha, 1, keepdims=True) \n",
    "\n",
    "loss_function = loss_eq5  ########### use 5th MSE loss equ: loss_eq5, loss_eq4, loss_eq3, mse_loss\n",
    "\n",
    "loss = tf.reduce_mean(loss_function(Y, alpha, K, global_step, annealing_step))\n",
    "l2_loss = tf.losses.get_regularization_loss() * lmb\n",
    "loss_func = loss + l2_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss_func, global_step=global_step)\n",
    "\n",
    "match = tf.reshape(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1)), tf.float32),(-1,1))\n",
    "accuracy = tf.reduce_mean(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "# l2_loss = tf.losses.get_regularization_loss()\n",
    "# loss_func = loss + l2_loss*0\n",
    "\n",
    "\n",
    "# # optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss_func)\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 - 100% -0.000000) Training Loss: 0.0383 \t Accuracy: 0.1310\n",
      "Testing:\t  Loss: 0.0305 \t Accuracy: 0.1165\n",
      "epoch 2 - 100% -0.000000) Training Loss: 0.0306 \t Accuracy: 0.1340\n",
      "Testing:\t  Loss: 0.0307 \t Accuracy: 0.1165\n",
      "epoch 3 - 100% -0.000000) Training Loss: 0.0305 \t Accuracy: 0.1291\n",
      "Testing:\t  Loss: 0.0304 \t Accuracy: 0.1165\n",
      "epoch 4 - 100% -0.000000) Training Loss: 0.0303 \t Accuracy: 0.1222\n",
      "Testing:\t  Loss: 0.0305 \t Accuracy: 0.1165\n",
      "epoch 5 - 100% -0.000000) Training Loss: 0.0304 \t Accuracy: 0.1234\n",
      "Testing:\t  Loss: 0.0305 \t Accuracy: 0.1165\n",
      "epoch 6 - 100% -0.000000) Training Loss: 0.0304 \t Accuracy: 0.1246\n",
      "Testing:\t  Loss: 0.0305 \t Accuracy: 0.1165\n",
      "epoch 7 - 100% -0.000000) Training Loss: 0.0304 \t Accuracy: 0.1233\n",
      "Testing:\t  Loss: 0.0305 \t Accuracy: 0.1165\n",
      "epoch 8 - 100% -0.000000) Training Loss: 0.0305 \t Accuracy: 0.1229\n",
      "Testing:\t  Loss: 0.0305 \t Accuracy: 0.1165\n",
      "epoch 9 - 100% -0.000000) Training Loss: 0.0305 \t Accuracy: 0.1211\n",
      "Testing:\t  Loss: 0.0305 \t Accuracy: 0.1165\n",
      "epoch 10 - 100% -0.000000) Training Loss: 0.0305 \t Accuracy: 0.1220\n",
      "Testing:\t  Loss: 0.0306 \t Accuracy: 0.1165\n",
      "epoch 11 - 100% -0.000000) Training Loss: 0.0305 \t Accuracy: 0.1230\n",
      "Testing:\t  Loss: 0.0306 \t Accuracy: 0.1165\n",
      "epoch 12 - 100% -0.000000) Training Loss: 0.0305 \t Accuracy: 0.1250\n",
      "Testing:\t  Loss: 0.0306 \t Accuracy: 0.1165\n",
      "epoch 13 - 100% -0.000000) Training Loss: 0.0305 \t Accuracy: 0.1236\n",
      "Testing:\t  Loss: 0.0305 \t Accuracy: 0.1165\n",
      "epoch 14 - 100% -0.000000) Training Loss: 0.0305 \t Accuracy: 0.1224\n",
      "Testing:\t  Loss: 0.0305 \t Accuracy: 0.1165\n",
      "epoch 15 - 100% -0.000000) Training Loss: 0.0306 \t Accuracy: 0.1218\n",
      "Testing:\t  Loss: 0.0306 \t Accuracy: 0.1165\n",
      "epoch 16 - 100% -0.000000) Training Loss: 0.0306 \t Accuracy: 0.1205\n",
      "Testing:\t  Loss: 0.0305 \t Accuracy: 0.1165\n",
      "epoch 17 - 100% -0.000000) Training Loss: 0.0306 \t Accuracy: 0.1224\n",
      "Testing:\t  Loss: 0.0306 \t Accuracy: 0.1165\n",
      "epoch 18 - 100% -0.000000) Training Loss: 0.0306 \t Accuracy: 0.1220\n",
      "Testing:\t  Loss: 0.0306 \t Accuracy: 0.1165\n",
      "epoch 19 - 100% -0.000000) Training Loss: 0.0306 \t Accuracy: 0.1206\n",
      "Testing:\t  Loss: 0.0306 \t Accuracy: 0.1165\n",
      "epoch 20 - 100% -0.000000) Training Loss: 0.0306 \t Accuracy: 0.1205\n",
      "Testing:\t  Loss: 0.0306 \t Accuracy: 0.1165\n",
      "epoch 21 - 100% -0.000000) Training Loss: 0.0306 \t Accuracy: 0.1231\n",
      "Testing:\t  Loss: 0.0306 \t Accuracy: 0.1165\n",
      "epoch 22 - 100% -0.000000) Training Loss: 0.0306 \t Accuracy: 0.1217\n",
      "Testing:\t  Loss: 0.0306 \t Accuracy: 0.1165\n",
      "epoch 23 - 100% -0.000000) Training Loss: 0.0306 \t Accuracy: 0.1230\n",
      "Testing:\t  Loss: 0.0306 \t Accuracy: 0.1165\n",
      "epoch 24 - 100% -0.000000) Training Loss: 0.0306 \t Accuracy: 0.1221\n",
      "Testing:\t  Loss: 0.0306 \t Accuracy: 0.1165\n",
      "epoch 25 - 100% -0.000000) Training Loss: 0.0306 \t Accuracy: 0.1233\n",
      "Testing:\t  Loss: 0.0306 \t Accuracy: 0.1165\n",
      "epoch 26 - 100% -0.000000) Training Loss: 0.0306 \t Accuracy: 0.1237\n",
      "Testing:\t  Loss: 0.0306 \t Accuracy: 0.1165\n",
      "epoch 27 - 100% -0.000000) Training Loss: 0.0306 \t Accuracy: 0.1230\n",
      "Testing:\t  Loss: 0.0306 \t Accuracy: 0.1165\n",
      "epoch 28 - 13% -0.000000) \r"
     ]
    }
   ],
   "source": [
    "cost_train = []\n",
    "cost_test = []\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "\n",
    "session =  tf.Session(config=config)\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# with tf.Session(config=config) as session:\n",
    "#     tf.global_variables_initializer().run()\n",
    "\n",
    "for itr in range(training_iterations):  \n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "    for i in range(n_batches):\n",
    "        offset = (i * batch_size) % (Y_train.shape[0] - batch_size)\n",
    "        batch_x = X_train[offset:(offset + batch_size), :, :, :]\n",
    "        batch_y = Y_train[offset:(offset + batch_size), :]\n",
    "\n",
    "        _, c, acc = session.run([optimizer, loss_func, accuracy],feed_dict={X: batch_x, Y : batch_y, keep_prob:.5, annealing_step:100*n_batches})\n",
    "        print('epoch %d - %d%% -%f) '% (itr+1, (100*(i+1))//n_batches, acc), end='\\r' if i<n_batches-1 else '')\n",
    "        acc_list.append(acc)\n",
    "        loss_list.append(c)\n",
    "    train_acc = np.array(np.array(acc_list).mean())\n",
    "    train_loss = np.array(np.array(loss_list).sum())\n",
    "    print('Training Loss: %2.4f \\t Accuracy: %2.4f' % (train_loss/Y_train.shape[0], train_acc))\n",
    "\n",
    "\n",
    "#       Performance on testing dataset:\n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "    for i in range(n_batches_test):\n",
    "        offset = (i * batch_size) % (Y_test.shape[0] - batch_size)\n",
    "        batch_x = X_test[offset:(offset + batch_size), :, :, :]\n",
    "        batch_y = Y_test[offset:(offset + batch_size), :]\n",
    "\n",
    "        c, acc = session.run([loss_func, accuracy],feed_dict={X: batch_x, Y : batch_y, keep_prob:1.,  annealing_step:100*n_batches})\n",
    "#             print('epoch %d - %d%% -%f) '% (itr+1, (100*(i+1))//n_batches, c), end='\\r' if i<n_batches-1 else '')\n",
    "        acc_list.append(acc)\n",
    "        loss_list.append(c)\n",
    "    test_acc = np.array(np.array(acc_list).mean())\n",
    "    test_loss = np.array(np.array(loss_list).sum())\n",
    "    print('Testing:\\t  Loss: %2.4f \\t Accuracy: %2.4f' % (test_loss/Y_test.shape[0], test_acc))\n",
    "\n",
    "    cost_train.append(train_loss)\n",
    "    cost_test.append(test_loss)\n",
    "    acc_train.append(train_acc)\n",
    "    acc_test.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost_train\n",
    "# cost_test\n",
    "# acc_train\n",
    "# acc_test\n",
    "fig = plt.figure(figsize=(9,3))\n",
    "plt.plot(cost_train, 'r-')\n",
    "plt.plot(cost_test, 'b-')\n",
    "\n",
    "# plt.axis([0,training_iterations,0,np.max(cost_history)])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(9,3))\n",
    "plt.plot(acc_train, 'r-')\n",
    "plt.plot(acc_test, 'b-')\n",
    "# plt.axis([0,training_iterations,0,np.max(cost_history)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_list = np.zeros(0)\n",
    "\n",
    "acc_list = []\n",
    "loss_list = []\n",
    "\n",
    "#       Performance on testing dataset:\n",
    "for i in range(n_batches_test):\n",
    "    if i == n_batches_test-1:\n",
    "        batch_x = X_test[i * batch_size:, :, :, :]\n",
    "        batch_y = Y_test[i * batch_size:, :]\n",
    "    else:\n",
    "        offset = (i * batch_size) % (Y_test.shape[0] - batch_size)\n",
    "        batch_x = X_test[offset:(offset + batch_size), :, :, :]\n",
    "        batch_y = Y_test[offset:(offset + batch_size), :]\n",
    "\n",
    "#     logits, y_pred = session.run([logits, prediction], feed_dict={X: batch_x, Y : batch_y})\n",
    "    y_pred, acc, c = session.run([prediction, accuracy, loss_func], feed_dict={X: batch_x, Y : batch_y})\n",
    "\n",
    "    print('epoch %d - %d%% -%f) '% (i+1, (100*(i+1))//n_batches_test, acc), end='\\r' if i<n_batches_test-1 else '')\n",
    "#     y_pred = np.argmax(logits, axis=1)\n",
    "#     pred_y_list.append(y_pred)\n",
    "    pred_y_list = np.concatenate([pred_y_list, y_pred])\n",
    "    \n",
    "    acc_list.append(acc)\n",
    "    loss_list.append(c)\n",
    "#     test_acc = np.array(np.array(acc_list).mean())\n",
    "#     test_loss = np.array(np.array(loss_list).sum())\n",
    "    \n",
    "\n",
    "test_acc = np.array(np.array(acc_list).mean())\n",
    "test_loss = np.array(np.array(loss_list).sum())\n",
    "print('Testing:\\t  Loss: %2.4f \\t Accuracy: %2.4f' % (test_loss/Y_test.shape[0], test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.argmax(Y_test, 1)\n",
    "y_pred =pred_y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "\n",
    "print('Accuracy on testing data:',sum(y_pred==y_true)/y_true.shape[0])\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(cf_matrix)\n",
    "class_wise_f1 = np.round(f1_score(y_true, y_pred, average=None)*100)*0.01\n",
    "print('the mean-f1 score: {:.2f}'.format(np.mean(class_wise_f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
