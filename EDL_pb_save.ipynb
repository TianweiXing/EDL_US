{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianwei/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "# plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model info:\n",
    "\n",
    "# _________________________________________________________________\n",
    "# Layer (type)                 Output Shape              Param #   \n",
    "# =================================================================\n",
    "# conv2d_1 (Conv2D)            (None, 38, 38, 32)        896       \n",
    "# _________________________________________________________________\n",
    "# activation_1 (Activation)    (None, 38, 38, 32)        0         \n",
    "# _________________________________________________________________\n",
    "# conv2d_2 (Conv2D)            (None, 36, 36, 32)        9248      \n",
    "# _________________________________________________________________\n",
    "# activation_2 (Activation)    (None, 36, 36, 32)        0         \n",
    "# _________________________________________________________________\n",
    "# max_pooling2d_1 (MaxPooling2 (None, 18, 18, 32)        0         \n",
    "# _________________________________________________________________\n",
    "# dropout_1 (Dropout)          (None, 18, 18, 32)        0         \n",
    "# _________________________________________________________________\n",
    "# conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
    "# _________________________________________________________________\n",
    "# activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
    "# _________________________________________________________________\n",
    "# conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
    "# _________________________________________________________________\n",
    "# activation_4 (Activation)    (None, 14, 14, 64)        0         \n",
    "# _________________________________________________________________\n",
    "# max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
    "# _________________________________________________________________\n",
    "# dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
    "# _________________________________________________________________\n",
    "# flatten_1 (Flatten)          (None, 3136)              0         \n",
    "# _________________________________________________________________\n",
    "# dense_1 (Dense)              (None, 256)               803072    \n",
    "# _________________________________________________________________\n",
    "# activation_5 (Activation)    (None, 256)               0         \n",
    "# _________________________________________________________________\n",
    "# dropout_3 (Dropout)          (None, 256)               0         \n",
    "# _________________________________________________________________\n",
    "# dense_2 (Dense)              (None, 10)                2570      \n",
    "# _________________________________________________________________\n",
    "# activation_6 (Activation)    (None, 10)                0         \n",
    "# =================================================================\n",
    "# Total params: 871,210\n",
    "# Trainable params: 871,210\n",
    "# Non-trainable params: 0\n",
    "# _________________________________________________________________\n",
    "\n",
    "# 0.9M params, about 4MB (keras 10MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_evidence(logits): \n",
    "    return tf.exp(tf.clip_by_value(logits,-10,10))\n",
    "\n",
    "#### KL Divergence calculator\n",
    "\n",
    "def KL(alpha, K):\n",
    "    beta=tf.constant(np.ones((1,K)),dtype=tf.float32)\n",
    "    S_alpha = tf.reduce_sum(alpha,axis=1,keepdims=True)\n",
    "    \n",
    "    KL = tf.reduce_sum((alpha - beta)*(tf.digamma(alpha)-tf.digamma(S_alpha)),axis=1,keepdims=True) + \\\n",
    "         tf.lgamma(S_alpha) - tf.reduce_sum(tf.lgamma(alpha),axis=1,keepdims=True) + \\\n",
    "         tf.reduce_sum(tf.lgamma(beta),axis=1,keepdims=True) - tf.lgamma(tf.reduce_sum(beta,axis=1,keepdims=True))\n",
    "    return KL\n",
    "\n",
    "def mse_loss(p, alpha, K, global_step, annealing_step): \n",
    "    S = tf.reduce_sum(alpha, axis=1, keep_dims=True) \n",
    "    E = alpha - 1\n",
    "    m = alpha / S\n",
    "    \n",
    "    A = tf.reduce_sum((p-m)**2, axis=1, keep_dims=True) \n",
    "    B = tf.reduce_sum(alpha*(S-alpha)/(S*S*(S+1)), axis=1, keep_dims=True) \n",
    "    \n",
    "    annealing_coef = tf.minimum(1.0,tf.cast(global_step/annealing_step,tf.float32))\n",
    "    \n",
    "    alp = E*(1-p) + 1 \n",
    "    C =  annealing_coef * KL(alp, K)\n",
    "    return (A + B) + C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-f516c369830a>:14: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "# new network:\n",
    "\n",
    "# NETWORK PARAMETERS\n",
    "data_w = 40\n",
    "data_h = 40\n",
    "n_classes = 10\n",
    "n_filters_1 = 32\n",
    "n_filters_2 = 64\n",
    "d_filter = 3\n",
    "p_drop_1 = 0.25\n",
    "p_drop_2 = 0.50\n",
    "\n",
    "batch_size = 256\n",
    "nb_epoch = 20\n",
    "\n",
    "K= n_classes\n",
    "num_channels = 3\n",
    "num_labels = n_classes\n",
    "\n",
    "lmb = 0.00\n",
    "omega = 1.0\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=omega)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None,data_w,data_h,num_channels], name = 'input')\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels], name = 'label')\n",
    "\n",
    "keep_prob = tf.placeholder(dtype=tf.float32, name = 'dropout_rate')\n",
    "global_step = tf.Variable(initial_value=0, name='global_step', trainable=False)\n",
    "annealing_step = tf.placeholder(dtype=tf.int32, name = 'annealing_step') \n",
    "\n",
    "### conv module\n",
    "\n",
    "# Convolutional Layer #1\n",
    "conv1 = tf.layers.conv2d(\n",
    "    inputs=X,\n",
    "    filters=32,\n",
    "    strides=(1, 1),\n",
    "    kernel_size=[3, 3],\n",
    "    kernel_regularizer=regularizer,\n",
    "    padding=\"valid\"\n",
    "    )\n",
    "conv1_act = tf.nn.relu( conv1 )\n",
    "# pool1 = tf.layers.max_pooling2d(inputs=act1, pool_size=[3, 3], strides=3)\n",
    "# dropout1 = tf.layers.dropout(\n",
    "#     inputs=pool1, rate=0.1)\n",
    "\n",
    "# Convolutional Layer #2\n",
    "conv2 = tf.layers.conv2d(\n",
    "    inputs=conv1_act,\n",
    "    filters=32,\n",
    "    strides=(1, 1),\n",
    "    kernel_size=[3, 3],\n",
    "    kernel_regularizer=regularizer,\n",
    "    padding=\"valid\"\n",
    "    )\n",
    "# bn2 = tf.layers.batch_normalization(\n",
    "#     conv2,\n",
    "#     axis=-1\n",
    "#     )\n",
    "conv2_act = tf.nn.relu( conv2 )\n",
    "conv2_mp = tf.layers.max_pooling2d(inputs=conv2_act, pool_size=[2, 2], strides=2)\n",
    "dpout1 = tf.layers.dropout(\n",
    "    inputs=conv2_mp, rate= p_drop_1)\n",
    "\n",
    "\n",
    "\n",
    "# Convolutional Layer #3\n",
    "conv3 = tf.layers.conv2d(\n",
    "    inputs=dpout1,\n",
    "    filters=64,\n",
    "    strides=(1, 1),\n",
    "    kernel_size=[3, 3],\n",
    "    kernel_regularizer=regularizer,\n",
    "    padding=\"valid\"\n",
    "    )\n",
    "conv3_act = tf.nn.relu( conv3 )\n",
    "\n",
    "# Convolutional Layer #4\n",
    "conv4 = tf.layers.conv2d(\n",
    "    inputs=conv1_act,\n",
    "    filters=64,\n",
    "    strides=(1, 1),\n",
    "    kernel_size=[3, 3],\n",
    "    kernel_regularizer=regularizer,\n",
    "    padding=\"valid\"\n",
    "    )\n",
    "# bn2 = tf.layers.batch_normalization(\n",
    "#     conv2,\n",
    "#     axis=-1\n",
    "#     )\n",
    "conv4_act = tf.nn.relu( conv4 )\n",
    "conv4_mp = tf.layers.max_pooling2d(inputs=conv4_act, pool_size=[2, 2], strides=2)\n",
    "dpout2 = tf.layers.dropout(\n",
    "    inputs=conv4_mp, rate= p_drop_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### modify dimensions\n",
    "shape = dpout2.get_shape().as_list()\n",
    "flat1 = tf.reshape(dpout2, [-1, shape[1] * shape[2]* shape[3]])\n",
    "\n",
    "\n",
    "\n",
    "### dense module\n",
    "\n",
    "fc1 = tf.layers.dense(inputs=flat1, \n",
    "                          kernel_regularizer=regularizer,\n",
    "                          units=256)\n",
    "fc1_act = tf.nn.relu( fc1 )\n",
    "dpout3 = tf.layers.dropout(\n",
    "    inputs=fc1_act, rate= p_drop_2)\n",
    "\n",
    "# Logits Layer\n",
    "logits = tf.layers.dense(inputs=dpout3, \n",
    "                         kernel_regularizer=regularizer,\n",
    "                         units=n_classes,\n",
    "                        name = 'logits_tensor')\n",
    "# actually name here is \"logits_tensor/BiasAdd\"\n",
    "# because this layer is performing matmul, biasadd, (no activation)\n",
    "# the cleanest way in ths future is that define the final layer as a layer with only single function.\n",
    "\n",
    "\n",
    "y_ = tf.nn.softmax(logits,name=\"softmax_tensor\")\n",
    "\n",
    "\n",
    "prediction = tf.argmax(logits, 1)\n",
    "\n",
    "\n",
    "\n",
    "########### EDL extension ###########\n",
    " \n",
    "logits2evidence =  exp_evidence ############ modify this function:  relu_evidence  exp_evidence softplus\n",
    "\n",
    "evidence = logits2evidence(logits)\n",
    "alpha = evidence + 1\n",
    "\n",
    "u = K / tf.reduce_sum(alpha, axis=1, keepdims=True)\n",
    "\n",
    "prob = alpha/tf.reduce_sum(alpha, 1, keepdims=True) \n",
    "\n",
    "loss_function = mse_loss  ########### use 5th MSE loss equ: loss_eq5, loss_eq4, loss_eq3, mse_loss\n",
    "\n",
    "loss = tf.reduce_mean(loss_function(Y, alpha, K, global_step, annealing_step))\n",
    "l2_loss = tf.losses.get_regularization_loss() * lmb\n",
    "loss_func = loss + l2_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss_func, global_step=global_step)\n",
    "\n",
    "match = tf.reshape(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1)), tf.float32),(-1,1))\n",
    "accuracy = tf.reduce_mean(match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from SaveModel/best_model.ckpt\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "# best_model_path = \"save_model/model_test1.ckpt\"\n",
    "best_model_path = \"SaveModel/best_model.ckpt\"\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "session1 =  tf.Session()\n",
    "saver.restore(session1, best_model_path)\n",
    "print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model to .pb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference\n",
    "\n",
    "# https://leimao.github.io/blog/Save-Load-Inference-From-TF-Frozen-Graph/\n",
    "# https://github.com/leimao/Frozen_Graph_TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.tools import freeze_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = saver\n",
    "directory = 'SaveModel'\n",
    "filename ='pb_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_filepath = \"SaveModel/best_model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pb_model.pbtxt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbtxt_filename = filename + '.pbtxt'\n",
    "pbtxt_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SaveModel/pb_model.pbtxt SaveModel/pb_model.pb\n"
     ]
    }
   ],
   "source": [
    "pbtxt_filepath = os.path.join(directory, pbtxt_filename)\n",
    "pb_filepath = os.path.join(directory, filename + '.pb')\n",
    "print(pbtxt_filepath, pb_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SaveModel/pb_model.pbtxt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.write_graph(graph_or_graph_def=session1.graph_def, logdir=directory, name=pbtxt_filename, as_text=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(([n.name for n in tf.get_default_graph().as_graph_def().node]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from SaveModel/best_model.ckpt\n",
      "INFO:tensorflow:Froze 8 variables.\n",
      "INFO:tensorflow:Converted 8 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph.freeze_graph(input_graph=pbtxt_filepath, \n",
    "                              input_saver='', \n",
    "                              input_binary=False, \n",
    "                              input_checkpoint=ckpt_filepath, \n",
    "                              output_node_names='logits_tensor/BiasAdd', \n",
    "                              restore_op_name='save/restore_all', \n",
    "                              filename_tensor_name='save/Const:0', \n",
    "                              output_graph=pb_filepath, \n",
    "                              clear_devices=True, \n",
    "                              initializer_nodes='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another method?\n",
    "\n",
    "# graph = tf.get_default_graph()\n",
    "# input_graph_def = graph.as_graph_def()\n",
    "# output_node_names = ['softmax_tensor', 'softmax_tensor']\n",
    "\n",
    "# output_graph_def = graph_util.convert_variables_to_constants(self.sess, input_graph_def, output_node_names)\n",
    "\n",
    "# with tf.gfile.GFile(pb_filepath, 'wb') as f:\n",
    "#     f.write(output_graph_def.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load .pb model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SaveModel/pb_model.pb\n"
     ]
    }
   ],
   "source": [
    "model_filepath = pb_filepath\n",
    "print(model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Check out the input placeholders:\n",
      "input => Placeholder\n"
     ]
    }
   ],
   "source": [
    "# tf.InteractiveSession.close()\n",
    "'''\n",
    "Lode trained model.\n",
    "'''\n",
    "print('Loading model...')\n",
    "graph = tf.Graph()\n",
    "sess = tf.InteractiveSession(graph = graph)\n",
    "\n",
    "with tf.gfile.GFile(model_filepath, 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "print('Check out the input placeholders:')\n",
    "nodes = [n.name + ' => ' +  n.op for n in graph_def.node if n.op in ('Placeholder')]\n",
    "for node in nodes:\n",
    "    print(node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading complete!\n"
     ]
    }
   ],
   "source": [
    "# Define input tensor\n",
    "model_input = tf.placeholder(tf.float32, shape=[None,40,40,3], name = 'input')\n",
    "\n",
    "\n",
    "# model_input = tf.placeholder(np.float32, shape = [None, 32, 32, 3], name='input')\n",
    "# dropout_rate = tf.placeholder(tf.float32, shape = [], name = 'dropout_rate')\n",
    "\n",
    "tf.import_graph_def(graph_def, {'input': model_input})\n",
    "\n",
    "print('Model loading complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "import/input\n",
      "import/conv2d/kernel\n",
      "import/conv2d/kernel/read\n",
      "import/conv2d/bias\n",
      "import/conv2d/bias/read\n",
      "import/conv2d/Conv2D\n",
      "import/conv2d/BiasAdd\n",
      "import/Relu\n",
      "import/conv2d_3/kernel\n",
      "import/conv2d_3/kernel/read\n",
      "import/conv2d_3/bias\n",
      "import/conv2d_3/bias/read\n",
      "import/conv2d_3/Conv2D\n",
      "import/conv2d_3/BiasAdd\n",
      "import/Relu_3\n",
      "import/max_pooling2d_1/MaxPool\n",
      "import/dropout_1/Identity\n",
      "import/Reshape/shape\n",
      "import/Reshape\n",
      "import/dense/kernel\n",
      "import/dense/kernel/read\n",
      "import/dense/bias\n",
      "import/dense/bias/read\n",
      "import/dense/MatMul\n",
      "import/dense/BiasAdd\n",
      "import/Relu_4\n",
      "import/dropout_2/Identity\n",
      "import/logits_tensor/kernel\n",
      "import/logits_tensor/kernel/read\n",
      "import/logits_tensor/bias\n",
      "import/logits_tensor/bias/read\n",
      "import/logits_tensor/MatMul\n",
      "import/logits_tensor/BiasAdd\n",
      "Name of the node - conv2d/kernel\n",
      "Value - \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tensor_util' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1338c87d333c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Name of the node - %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Value - \"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeNdarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tensor_util' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get layer names\n",
    "layers = [op.name for op in graph.get_operations()]\n",
    "for layer in layers:\n",
    "    print(layer)\n",
    "\n",
    "# Check out the weights of the nodes\n",
    "weight_nodes = [n for n in graph_def.node if n.op == 'Const']\n",
    "for n in weight_nodes:\n",
    "    print(\"Name of the node - %s\" % n.name)\n",
    "    print(\"Value - \" )\n",
    "    print(tensor_util.MakeNdarray(n.attr['value'].tensor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Know your output node name\n",
    "output_tensor = graph.get_tensor_by_name(\"import/logits_tensor/BiasAdd:0\")\n",
    "# output = sess.run(output_tensor, feed_dict =  {model_input:  X_test[0:2] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing results of 2 ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_class):\n",
    "    n_labels = len(labels)\n",
    "#     n_unique_labels = len(np.unique(labels))\n",
    "    n_unique_labels = num_class\n",
    "    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading data\n",
      "Data loaded\n",
      "(54154, 40, 40, 3)\n",
      "(54154, 10)\n",
      "For batch of size 256: \n",
      " 211 batches in test\n"
     ]
    }
   ],
   "source": [
    "## GET DATA TO WORK ON\n",
    "print(\"Start loading data\")\n",
    "\n",
    "fd = open(\"data_x.pkl\", 'rb')\n",
    "fd2 = open(\"data_y.pkl\", 'rb')\n",
    "features = pickle.load(fd)\n",
    "labels = pickle.load(fd2)\n",
    "\n",
    "print(\"Data loaded\")\n",
    "\n",
    "# all testing data\n",
    "X_test = features[:]\n",
    "Y_test = labels[:]\n",
    "\n",
    "X_test = X_test.astype('float32')\n",
    "X_test /= 255\n",
    "\n",
    "## one hot encoding\n",
    "Y_test = one_hot_encode(Y_test, 10)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "n_batches_test = Y_test.shape[0]//batch_size\n",
    "print('For batch of size %d: \\n %d batches in test'%(batch_size, n_batches_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pb model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10.591967 , -40.03761  , -11.062835 ,  -9.509928 , -25.767548 ,\n",
       "        -11.834668 , -22.799807 , -22.827818 ,  -5.7495046, -34.31049  ],\n",
       "       [ 17.028324 , -40.26766  ,  -7.9120092, -14.365061 , -26.771418 ,\n",
       "        -15.199751 , -27.301739 , -26.882553 ,  -8.996705 , -33.614563 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb_output = sess.run(output_tensor, feed_dict =  {model_input:  X_test[0:2] })\n",
    "pb_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00045379]\n",
      " [0.00045379]]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "def predict_label(logits_output):\n",
    "    predict_y = np.argmax(logits_output, axis = 1)\n",
    "    return predict_y\n",
    "\n",
    "def exp_evidence(logits): \n",
    "    return np.exp(np.clip(logits,-10,10))\n",
    "\n",
    "def uncertainty_score( logits_output ):\n",
    "    evidence = exp_evidence(logits_output)\n",
    "    alpha = evidence + 1\n",
    "    u_score = 10 / np.sum(alpha, axis=1, keepdims=True)  # K = num_classes = 10\n",
    "    return u_score\n",
    "\n",
    "print(uncertainty_score(pb_output))\n",
    "\n",
    "print(predict_label(pb_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [[0.00045379]\n",
      " [0.00045379]]\n",
      "[[ 10.591967  -40.03761   -11.062835   -9.509928  -25.767548  -11.834668\n",
      "  -22.799807  -22.827818   -5.7495046 -34.31049  ]\n",
      " [ 17.028324  -40.26766    -7.9120092 -14.365061  -26.771418  -15.199751\n",
      "  -27.301739  -26.882553   -8.996705  -33.614563 ]]\n"
     ]
    }
   ],
   "source": [
    "y_pred, u, logits = session1.run([prediction, u, logits], feed_dict={X: X_test[0:2], Y : Y_test[0:2], keep_prob:1.,  annealing_step:100*n_batches_test})\n",
    "\n",
    "print(y_pred,u)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About classes:\n",
    "# 0\tair_conditioner\n",
    "# 1\tcar_horn\n",
    "# 2\tchildren_playing\n",
    "# 3\tdog_bark\n",
    "# 4\tdrilling\n",
    "# 5\tengine_idling\n",
    "# 6\tgun_shot\n",
    "# 7\tjackhammer\n",
    "# 8\tsiren\n",
    "# 9\tstreet_music"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
